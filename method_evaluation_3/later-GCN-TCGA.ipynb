{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, label_binarize\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Project-specific imports (MoGoNet)\n",
        "from models.models_GCN import init_model_dict, init_optim\n",
        "from models.train_test_GCN import (\n",
        "    gen_trte_adj_mat, train_epoch, test_epoch\n",
        ")\n",
        "from models.utils import (\n",
        "    save_model_dict, one_hot_tensor, cal_sample_weight,\n",
        "    gen_adj_mat_tensor, gen_test_adj_mat_tensor, cal_adj_mat_parameter\n",
        ")\n",
        "from models.utils2 import evaluate_model, count_trainable_parameters\n",
        "\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AML 的类别数是: 3\n"
          ]
        }
      ],
      "source": [
        "cancer_class_mapping = {\n",
        "    \"AML\": 3,\n",
        "    \"BRCA\": 5,\n",
        "    \"COAD\": 4,\n",
        "    \"GBM\": 4,\n",
        "    \"KIRC\": 4,\n",
        "    \"LIHC\": 4,\n",
        "    \"LUSC\": 4,\n",
        "    \"OV\": 4,\n",
        "    \"SARC\": 4,\n",
        "    \"SKCM\": 2\n",
        "}\n",
        "\n",
        "cancer_type = \"AML\"\n",
        "class_num = cancer_class_mapping[cancer_type]\n",
        "print(f\"{cancer_type} 的类别数是: {class_num}\")  # 输出: BRCA 的类别数是: 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 参数设置\n",
        "scenario = 1\n",
        "nn_type = \"later-GCN\"\n",
        "# signal_prop = \"high\"\n",
        "# signal_level = \"high\"\n",
        "# batch_env = f\"s{scenario}-k3-{signal_prop}-{signal_level}\"\n",
        "result_path = f\"F:/r-env/大课题/模拟试验1_3/result/{cancer_type}/\"\n",
        "prediction_path = f\"F:/r-env/大课题/模拟试验1_3/predictions/{cancer_type}/\"\n",
        "# 创建目录（如果不存在）\n",
        "os.makedirs(result_path, exist_ok=True)\n",
        "os.makedirs(prediction_path, exist_ok=True)\n",
        "# labels = np.loadtxt(f\"F:/r-env/中期/方法/模拟试验-模拟数据/情景{scenario}/k-3/label-k3.csv\", delimiter=',')\n",
        "cuda = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View and class settings\n",
        "view_list = [1, 2, 3]  # List of views (e.g., modalities)\n",
        "num_view = len(view_list)  # Number of views\n",
        "num_class = 3  # Number of classes\n",
        "# Model dimensionality settings\n",
        "# dim_hvcdn = pow(num_class, num_view)  # Dimension of the H-VCDN\n",
        "dim_hvcdn = 100 \n",
        "dim_he_list = [300, 200, 100]  # List of dimensions for hidden embeddings\n",
        "# Adjacency matrix settings\n",
        "adj_parameter = 4  # Parameter controlling adjacency matrix generation\n",
        "# Training and learning rate settings\n",
        "num_epoch_pretrain = 30  # Number of epochs for pretraining\n",
        "num_epoch = 500  # Total number of training epochs\n",
        "lr_e_pretrain = 1e-3  # Learning rate for encoder during pretraining\n",
        "lr_e = 1e-3  # Learning rate for encoder during main training\n",
        "lr_c = 5e-2  # Learning rate for classifier during main training\n",
        "# Testing and evaluation settings\n",
        "test_interval = 2  # Interval for testing the model (in epochs)\n",
        "\n",
        "dropout_rate = 0.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=2023110400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始进行使用later-GCN对AML进行分类的实验\n",
            "开始第 1 次模拟实验\n",
            "CV1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 163\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# 预训练\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch_pretrain):\n\u001b[1;32m--> 163\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_tr_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_tr_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_tr_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m                \u001b[49m\u001b[43monehot_labels_tr_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_VCDN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# 主训练\u001b[39;00m\n\u001b[0;32m    167\u001b[0m optim_dict \u001b[38;5;241m=\u001b[39m init_optim(num_view, model_dict, lr_e, lr_c)\n",
            "File \u001b[1;32mf:\\r-env\\大课题\\模拟试验1_3\\mogonet\\train_test_GCN.py:70\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(data_list, adj_list, label, one_hot_label, sample_weight, model_dict, optim_dict, train_VCDN)\u001b[0m\n\u001b[0;32m     68\u001b[0m optim_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{:}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     69\u001b[0m ci_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 70\u001b[0m ci \u001b[38;5;241m=\u001b[39m model_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{:}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)](\u001b[43mmodel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE\u001b[39;49m\u001b[38;5;132;43;01m{:}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43madj_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     71\u001b[0m ci_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mmul(criterion(ci, label),sample_weight))\n\u001b[0;32m     72\u001b[0m ci_loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# 反向传播，计算梯度\u001b[39;00m\n",
            "File \u001b[1;32mf:\\conda-env\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mf:\\conda-env\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mf:\\r-env\\大课题\\模拟试验1_3\\mogonet\\models_GCN.py:45\u001b[0m, in \u001b[0;36mGCN_E.forward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[1;32m---> 45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(x, \u001b[38;5;241m0.25\u001b[39m)\n\u001b[0;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
            "File \u001b[1;32mf:\\conda-env\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mf:\\conda-env\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mf:\\r-env\\大课题\\模拟试验1_3\\mogonet\\models_GCN.py:28\u001b[0m, in \u001b[0;36mGraphConvolution.forward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[1;32m---> 28\u001b[0m     support \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mmm(adj, support)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# 设置信号比例和信号水平的组合\n",
        "signal_props = [\"1\"]\n",
        "signal_levels = [\"1\"]\n",
        "# signal_levels = [\"mid\"]\n",
        "# signal_dict = {\n",
        "#     \"high\": \"高\",\n",
        "#     \"mid\": \"中\",\n",
        "#     \"low\": \"低\"\n",
        "# }\n",
        "# signal_props = [\"high\"]\n",
        "# signal_levels = [\"high\", \"mid\"]\n",
        "\n",
        "# 保存每次实验的运行时间\n",
        "run_times = []\n",
        "\n",
        "# 创建一个组合的列表，并循环每个组合\n",
        "for signal_prop in signal_props:\n",
        "    for signal_level in signal_levels:\n",
        "        # 获取中文的信号比例和信号水平\n",
        "        # ch_sig_prop = signal_dict.get(signal_prop, \"未知\")\n",
        "        # ch_sig_level = signal_dict.get(signal_level, \"未知\")\n",
        "\n",
        "        # 设置当前的 batch_env\n",
        "        batch_env = f\"s{scenario}-{cancer_type}-k{num_class}\"\n",
        "\n",
        "        # 打印当前的组合\n",
        "        print(f\"开始进行使用{nn_type}对{cancer_type}进行分类的实验\")\n",
        "\n",
        "        metrics = {\"Batch\": [], \"CV\": [], \"Accuracy\": [], \"F1 Macro\": [], \"F1 Micro\": [], \"F1 Weighted\": [],\n",
        "                   \"Precision Macro\": [], \"Precision Micro\": [], \"Precision Weighted\": [],\n",
        "                   \"Recall Macro\": [], \"Recall Micro\": [], \"Recall Weighted\": [],\n",
        "                   \"AUC Macro\": [], \"AUC Micro\": [], \"AUC Weighted\": [],\n",
        "                   \"AUPR Macro\": [], \"AUPR Micro\": [], \"AUPR Weighted\": [], \"Cohen Kappa\": [], \"Training_time\": [], \"GPU_memory_allocated\":[]}\n",
        "        \n",
        "        predictions_data = {\n",
        "            \"batch_num\": [],\n",
        "            \"fold_num\": [],\n",
        "            \"y_true\": [],\n",
        "            \"y_pred\": []\n",
        "        }\n",
        "\n",
        "        # 动态添加概率字段（根据 num_class）\n",
        "        for i in range(1, num_class + 1):\n",
        "            predictions_data[f\"proba_{i}\"] = []    \n",
        "        # 记录实验的开始时间\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 你的实验循环代码\n",
        "        for batch_num in range(1, 2):\n",
        "            print(f\"开始第 {batch_num} 次模拟实验\")\n",
        "\n",
        "            # 数据路径\n",
        "            sim_path = f\"F:/r-env/data/benchmark-TCGA\"\n",
        "            # checkpoint_path = f\"F:/r-env/中期/方法/模拟试验-模型参数/情景{scenario}/k-3/信号比例-{ch_sig_prop}/信号水平-{ch_sig_level}/sim{batch_num}\"\n",
        "\n",
        "            omics1 = pd.read_csv(f\"{sim_path}/{cancer_type}/exp.csv\", header = 0)\n",
        "            omics2 = pd.read_csv(f\"{sim_path}/{cancer_type}/meth.csv\", header = 0)\n",
        "            omics3 = pd.read_csv(f\"{sim_path}/{cancer_type}/mirna.csv\", header = 0)\n",
        "\n",
        "            omics1 = omics1.iloc[:, 1:].to_numpy() \n",
        "            omics2 = omics2.iloc[:, 1:].to_numpy() \n",
        "            omics3 = omics3.iloc[:, 1:].to_numpy() \n",
        "\n",
        "            df_label = pd.read_csv(f\"{sim_path}/{cancer_type}/label.csv\")\n",
        "            labels = df_label['label'].values \n",
        "\n",
        "            omics_data = [omics1, omics2, omics3]\n",
        "\n",
        "            for fold_num, (train_idx_raw, test_idx) in enumerate(kfold.split(omics1, labels)):\n",
        "                print(f\"CV{fold_num + 1}\")\n",
        "\n",
        "                # 将train_idx_raw按3:1划分为train_idx和val_idx\n",
        "                train_idx, val_idx = train_test_split(\n",
        "                    train_idx_raw, \n",
        "                    test_size=0.25,  # 25%作为验证集，75%作为训练集 (3:1比例)\n",
        "                    random_state=42,\n",
        "                    stratify=labels[train_idx_raw]  # 保持标签分布\n",
        "                )\n",
        "\n",
        "                \n",
        "                data_tr_list = []\n",
        "                data_val_list = []\n",
        "                data_te_list = []\n",
        "                data_trval_list = []\n",
        "                data_trte_list = []\n",
        "\n",
        "                # 通过循环处理每个omics数据集\n",
        "                for omic in omics_data:\n",
        "                    train_X = omic[train_idx]\n",
        "                    val_X = omic[val_idx]\n",
        "                    test_X = omic[test_idx]\n",
        "                    data_tr_list.append(torch.FloatTensor(train_X).cuda() )\n",
        "                    data_val_list.append(torch.FloatTensor(val_X).cuda() )\n",
        "                    data_te_list.append(torch.FloatTensor(test_X).cuda() )\n",
        "                    data_trval_list.append(torch.FloatTensor(np.concatenate((train_X, val_X), axis=0)).cuda() )\n",
        "                    data_trte_list.append(torch.FloatTensor(np.concatenate((train_X, test_X), axis=0)).cuda() )\n",
        "\n",
        "                train_y, val_y, test_y = labels[train_idx], labels[val_idx], labels[test_idx]\n",
        "\n",
        "                # model_filepath = os.path.join(checkpoint_path, f\"early-GCN-model{fold_num + 1}\")\n",
        "                # os.makedirs(model_filepath, exist_ok=True)\n",
        "                # checkpoint = ModelCheckpoint(model_filepath, save_weights_only=True, monitor='val_acc', save_best_only=True, mode='max')\n",
        "\n",
        "                # View and class settings\n",
        "\n",
        "                # torch.cuda.empty_cache()\n",
        "\n",
        "                # 准备数据\n",
        "                # data_tr_list = [torch.FloatTensor(train_X)]\n",
        "                # data_trval_list = [torch.FloatTensor(np.concatenate((train_X, val_X), axis=0))]\n",
        "                # data_trte_list = [torch.FloatTensor(np.concatenate((train_X, test_X), axis=0))]\n",
        "\n",
        "                if cuda:\n",
        "                    data_tr_list[0] = data_tr_list[0].cuda()\n",
        "                    data_trval_list[0] = data_trval_list[0].cuda()\n",
        "                    data_trte_list[0] = data_trte_list[0].cuda()\n",
        "\n",
        "                num_tr = data_tr_list[0].shape[0]\n",
        "                num_trval = data_trval_list[0].shape[0]\n",
        "                num_trte = data_trte_list[0].shape[0]\n",
        "\n",
        "                labels_trval = np.concatenate((train_y, val_y))\n",
        "                labels_trte = np.concatenate((train_y, test_y))\n",
        "\n",
        "                trval_idx = {\n",
        "                    \"tr\": list(range(num_tr)),\n",
        "                    \"te\": list(range(num_tr, num_trval))\n",
        "                }\n",
        "                \n",
        "                trte_idx = {\n",
        "                    \"tr\": list(range(num_tr)),\n",
        "                    \"te\": list(range(num_tr, num_trte))\n",
        "                }\n",
        "                \n",
        "                labels_tr_tensor = torch.LongTensor(labels_trval[trval_idx[\"tr\"]])\n",
        "                onehot_labels_tr_tensor = one_hot_tensor(labels_tr_tensor, num_class)\n",
        "                sample_weight_tr = torch.FloatTensor(cal_sample_weight(labels_trval[trval_idx[\"tr\"]], num_class))\n",
        "                \n",
        "                if cuda:\n",
        "                    labels_tr_tensor = labels_tr_tensor.cuda()\n",
        "                    onehot_labels_tr_tensor = onehot_labels_tr_tensor.cuda()\n",
        "                    sample_weight_tr = sample_weight_tr.cuda()\n",
        "\n",
        "                dim_list = [x.shape[1] for x in data_tr_list]\n",
        "\n",
        "                adj_tr_list, adj_val_list = gen_trte_adj_mat(data_tr_list, data_trval_list, trval_idx, adj_parameter)\n",
        "\n",
        "                # 初始化模型\n",
        "                model_dict = init_model_dict(num_view, num_class, dim_list, dim_he_list, dim_hvcdn, dropout_rate)\n",
        "                optim_dict = init_optim(num_view, model_dict, lr_e_pretrain, lr_c)\n",
        "                \n",
        "                for model in model_dict.values():\n",
        "                    if cuda:\n",
        "                        model.cuda()\n",
        "\n",
        "                \n",
        "                # 记录单个fold的开始时间\n",
        "                fold_start_time = time.time()\n",
        "\n",
        "\n",
        "                # 预训练\n",
        "                for epoch in range(num_epoch_pretrain):\n",
        "                    train_epoch(data_tr_list, adj_tr_list, labels_tr_tensor, \n",
        "                                onehot_labels_tr_tensor, sample_weight_tr, model_dict, optim_dict, train_VCDN=True)\n",
        "                \n",
        "                # 主训练\n",
        "                optim_dict = init_optim(num_view, model_dict, lr_e, lr_c)\n",
        "\n",
        "                # 早停机制\n",
        "                best_accuracy = 0.0\n",
        "                patience = 25\n",
        "                no_improvement_count = 0\n",
        "                best_model = None\n",
        "\n",
        "\n",
        "                for epoch in range(num_epoch):\n",
        "                    train_epoch(data_tr_list,adj_tr_list, labels_tr_tensor, \n",
        "                               onehot_labels_tr_tensor, sample_weight_tr, model_dict, optim_dict, train_VCDN=True)\n",
        "                    \n",
        "                    # 每10个epoch验证一次\n",
        "                    if epoch % 2 == 0:\n",
        "                        val_prob = test_epoch(data_trval_list, adj_val_list,trval_idx[\"te\"], model_dict) ####\n",
        "                        predictions = np.argmax(val_prob, axis=1)\n",
        "                        accuracy = accuracy_score(val_y, predictions)\n",
        "                        \n",
        "                        if accuracy > best_accuracy:\n",
        "                            best_accuracy = accuracy\n",
        "                            no_improvement_count = 0\n",
        "                            best_model = copy.deepcopy(model_dict)\n",
        "                        else:\n",
        "                            no_improvement_count += 2\n",
        "                            \n",
        "                        if no_improvement_count >= patience:\n",
        "                            break\n",
        "                        # 记录单个fold的结束时间\n",
        "                # log_gpu_memory()\n",
        "                fold_end_time = time.time()\n",
        "                fold_elapsed_time = fold_end_time - fold_start_time\n",
        "\n",
        "                memory_allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "\n",
        "                # 最终测试\n",
        "                adj_tr_list, adj_te_list = gen_trte_adj_mat(data_tr_list, data_trte_list, trte_idx, adj_parameter)\n",
        "                predictions = test_epoch(data_trte_list, adj_te_list, trte_idx[\"te\"], best_model)\n",
        "                y_pred = np.argmax(predictions, axis=1)\n",
        "                # y_true = labels_trte[trte_idx[\"te\"]]    \n",
        "                y_true = test_y\n",
        "\n",
        "                prediction_to_add = {\n",
        "                    \"batch_num\": batch_num,\n",
        "                    \"fold_num\": fold_num + 1,\n",
        "                    \"y_true\": y_true.tolist(),\n",
        "                    \"y_pred\": y_pred.tolist()\n",
        "                }\n",
        "\n",
        "                # 动态添加概率预测（从 predictions 数组中按列提取）\n",
        "                for i in range(num_class):\n",
        "                    prediction_to_add[f\"proba_{i+1}\"] = predictions[:, i].tolist()\n",
        "\n",
        "                # 动态添加概率预测（从 predictions 数组中按列提取）\n",
        "                for key, values in prediction_to_add.items():\n",
        "                    if key in [\"batch_num\", \"fold_num\"]:\n",
        "                        # 对非列表字段（如batch_num），扩展为与样本数等长的列表\n",
        "                        predictions_data[key].extend([values] * len(y_true))\n",
        "                    else:\n",
        "                        # 对列表字段（如y_true, proba_1等），直接扩展\n",
        "                        predictions_data[key].extend(values) \n",
        "        \n",
        "\n",
        "                scores = evaluate_model(y_true, y_pred, predictions, class_num)\n",
        "\n",
        "                for key in metrics:\n",
        "                    if key == \"Batch\":\n",
        "                        metrics[key].append(batch_num)\n",
        "                    elif key == \"CV\":\n",
        "                        metrics[key].append(fold_num + 1)\n",
        "                    elif key == \"Training_time\":\n",
        "                        metrics[key].append(fold_elapsed_time)\n",
        "                    elif key == \"GPU_memory_allocated\":\n",
        "                        metrics[key].append(memory_allocated)  # 记录 GPU 内存占用\n",
        "                    else:\n",
        "                        metrics[key].append(scores.get(key, None))  # 没有的指标填None\n",
        "\n",
        "        end_time = time.time() \n",
        "\n",
        "        trainable_params = count_trainable_parameters(model_dict)\n",
        "        # print(f\"模型可训练参数数量: {trainable_params:,}\")\n",
        "        \n",
        "        metrics_df = pd.DataFrame(metrics)\n",
        "        float_cols = metrics_df.select_dtypes(include=['float64']).columns\n",
        "        metrics_df[float_cols] = metrics_df[float_cols].round(6)\n",
        "        result_filepath = os.path.join(result_path, f\"metrics-{nn_type}-{cancer_type}.csv\")\n",
        "        metrics_df.to_csv(result_filepath, index=False)\n",
        "\n",
        "        predictions_data = pd.DataFrame(predictions_data)\n",
        "        if not predictions_data.empty:\n",
        "            # 筛选需要保留4位小数的列\n",
        "            float_cols = [f'proba_{i+1}' for i in range(num_class)]  # 根据你的实际列名调整\n",
        "            # 应用四舍五入\n",
        "            predictions_data[float_cols] = predictions_data[float_cols].round(4)\n",
        "        predictions_filepath = os.path.join(prediction_path, f\"prediction-{nn_type}-{cancer_type}.csv\")\n",
        "        predictions_data.to_csv(predictions_filepath, index=False)\n",
        "        print(f\"当前批次实验完成，结果已保存到: {result_filepath}\")\n",
        "\n",
        "\n",
        "        elapsed_time = end_time - start_time  # 计算每次实验的运行时间\n",
        "        run_times.append({\n",
        "            # \"signal_prop\": signal_prop,\n",
        "            # \"signal_level\": signal_level,\n",
        "            \"training_num\": batch_num * 4,\n",
        "            \"elapsed_time\": elapsed_time,   \n",
        "            \"trainable_params\": trainable_params\n",
        "        })\n",
        "\n",
        "        \n",
        "# 将所有运行时间保存到 CSV 文件\n",
        "run_times_df = pd.DataFrame(run_times)\n",
        "result_filepath = f\"{result_path}/{nn_type}_{cancer_type}_实验运行时间.csv\"\n",
        "run_times_df.to_csv(result_filepath, index=False)\n",
        "\n",
        "print(f\"所有实验完成，运行时间已保存到: {result_filepath}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "开始进行使用later-GCN对COAD进行分类的实验，类别数为: 4\n",
            "开始第 1 次模拟实验\n",
            "CV1\n",
            "CV2\n",
            "CV3\n",
            "CV4\n",
            "当前癌症类型 COAD 实验完成，结果已保存到: F:/r-env/大课题/模拟试验1_3/result/COAD/metrics-later-GCN-COAD.csv\n",
            "癌症类型COAD实验完成，运行时间已保存到: F:/r-env/大课题/模拟试验1_3/result//COAD/later-GCN_COAD_实验运行时间.csv\n",
            "\n",
            "开始进行使用later-GCN对GBM进行分类的实验，类别数为: 4\n",
            "开始第 1 次模拟实验\n",
            "CV1\n",
            "CV2\n",
            "CV3\n",
            "CV4\n",
            "当前癌症类型 GBM 实验完成，结果已保存到: F:/r-env/大课题/模拟试验1_3/result/GBM/metrics-later-GCN-GBM.csv\n",
            "癌症类型GBM实验完成，运行时间已保存到: F:/r-env/大课题/模拟试验1_3/result//GBM/later-GCN_GBM_实验运行时间.csv\n",
            "\n",
            "开始进行使用later-GCN对KIRC进行分类的实验，类别数为: 4\n",
            "开始第 1 次模拟实验\n",
            "CV1\n",
            "CV2\n",
            "CV3\n",
            "CV4\n",
            "当前癌症类型 KIRC 实验完成，结果已保存到: F:/r-env/大课题/模拟试验1_3/result/KIRC/metrics-later-GCN-KIRC.csv\n",
            "癌症类型KIRC实验完成，运行时间已保存到: F:/r-env/大课题/模拟试验1_3/result//KIRC/later-GCN_KIRC_实验运行时间.csv\n",
            "\n",
            "开始进行使用later-GCN对LIHC进行分类的实验，类别数为: 4\n",
            "开始第 1 次模拟实验\n",
            "CV1\n",
            "CV2\n",
            "CV3\n",
            "CV4\n",
            "当前癌症类型 LIHC 实验完成，结果已保存到: F:/r-env/大课题/模拟试验1_3/result/LIHC/metrics-later-GCN-LIHC.csv\n",
            "癌症类型LIHC实验完成，运行时间已保存到: F:/r-env/大课题/模拟试验1_3/result//LIHC/later-GCN_LIHC_实验运行时间.csv\n",
            "\n",
            "开始进行使用later-GCN对LUSC进行分类的实验，类别数为: 4\n",
            "开始第 1 次模拟实验\n",
            "CV1\n",
            "CV2\n",
            "CV3\n",
            "CV4\n",
            "当前癌症类型 LUSC 实验完成，结果已保存到: F:/r-env/大课题/模拟试验1_3/result/LUSC/metrics-later-GCN-LUSC.csv\n",
            "癌症类型LUSC实验完成，运行时间已保存到: F:/r-env/大课题/模拟试验1_3/result//LUSC/later-GCN_LUSC_实验运行时间.csv\n",
            "\n",
            "开始进行使用later-GCN对OV进行分类的实验，类别数为: 4\n",
            "开始第 1 次模拟实验\n",
            "CV1\n",
            "CV2\n",
            "CV3\n",
            "CV4\n",
            "当前癌症类型 OV 实验完成，结果已保存到: F:/r-env/大课题/模拟试验1_3/result/OV/metrics-later-GCN-OV.csv\n",
            "癌症类型OV实验完成，运行时间已保存到: F:/r-env/大课题/模拟试验1_3/result//OV/later-GCN_OV_实验运行时间.csv\n",
            "\n",
            "开始进行使用later-GCN对SARC进行分类的实验，类别数为: 4\n",
            "开始第 1 次模拟实验\n",
            "CV1\n",
            "CV2\n",
            "CV3\n",
            "CV4\n",
            "当前癌症类型 SARC 实验完成，结果已保存到: F:/r-env/大课题/模拟试验1_3/result/SARC/metrics-later-GCN-SARC.csv\n",
            "癌症类型SARC实验完成，运行时间已保存到: F:/r-env/大课题/模拟试验1_3/result//SARC/later-GCN_SARC_实验运行时间.csv\n",
            "\n",
            "开始进行使用later-GCN对SKCM进行分类的实验，类别数为: 2\n",
            "开始第 1 次模拟实验\n",
            "CV1\n",
            "CV2\n",
            "CV3\n",
            "CV4\n",
            "当前癌症类型 SKCM 实验完成，结果已保存到: F:/r-env/大课题/模拟试验1_3/result/SKCM/metrics-later-GCN-SKCM.csv\n",
            "癌症类型SKCM实验完成，运行时间已保存到: F:/r-env/大课题/模拟试验1_3/result//SKCM/later-GCN_SKCM_实验运行时间.csv\n",
            "\n",
            "开始进行使用later-GCN对BRCA进行分类的实验，类别数为: 5\n",
            "开始第 1 次模拟实验\n",
            "CV1\n",
            "CV2\n",
            "CV3\n",
            "CV4\n",
            "当前癌症类型 BRCA 实验完成，结果已保存到: F:/r-env/大课题/模拟试验1_3/result/BRCA/metrics-later-GCN-BRCA.csv\n",
            "癌症类型BRCA实验完成，运行时间已保存到: F:/r-env/大课题/模拟试验1_3/result//BRCA/later-GCN_BRCA_实验运行时间.csv\n"
          ]
        }
      ],
      "source": [
        "# 设置信号比例和信号水平的组合\n",
        "signal_props = [\"1\"]\n",
        "signal_levels = [\"1\"]\n",
        "\n",
        "# 保存每次实验的运行时间\n",
        "\n",
        "result_path = f\"F:/r-env/大课题/模拟试验1_3/result/\"\n",
        "prediction_path = f\"F:/r-env/大课题/模拟试验1_3/predictions/\"\n",
        "\n",
        "# 癌症类型和对应类别数的映射\n",
        "cancer_class_mapping = {\n",
        "    # \"AML\": 3,\n",
        "    \"COAD\": 4,\n",
        "    \"GBM\": 4,\n",
        "    \"KIRC\": 4,\n",
        "    \"LIHC\": 4,\n",
        "    \"LUSC\": 4,\n",
        "    \"OV\": 4,\n",
        "    \"SARC\": 4,\n",
        "    \"SKCM\": 2,\n",
        "    \"BRCA\": 5\n",
        "}\n",
        "\n",
        "# 遍历所有癌症类型\n",
        "for cancer_type, class_num in cancer_class_mapping.items():\n",
        "    num_class = class_num\n",
        "    print(f\"\\n开始进行使用{nn_type}对{cancer_type}进行分类的实验，类别数为: {num_class}\")\n",
        "    \n",
        "    # 创建一个组合的列表，并循环每个组合\n",
        "    for signal_prop in signal_props:\n",
        "        for signal_level in signal_levels:\n",
        "            # 设置当前的 batch_env\n",
        "            batch_env = f\"s{scenario}-{cancer_type}-k{num_class}\"\n",
        "\n",
        "            run_times = []\n",
        "\n",
        "            metrics = {\"Batch\": [], \"CV\": [], \"Accuracy\": [], \"F1 Macro\": [], \"F1 Micro\": [], \"F1 Weighted\": [],\n",
        "                      \"Precision Macro\": [], \"Precision Micro\": [], \"Precision Weighted\": [],\n",
        "                      \"Recall Macro\": [], \"Recall Micro\": [], \"Recall Weighted\": [],\n",
        "                      \"AUC Macro\": [], \"AUC Micro\": [], \"AUC Weighted\": [],\n",
        "                      \"AUPR Macro\": [], \"AUPR Micro\": [], \"AUPR Weighted\": [], \"Cohen Kappa\": [], \n",
        "                      \"Training_time\": [], \"GPU_memory_allocated\":[]}\n",
        "            \n",
        "            predictions_data = {\n",
        "                \"batch_num\": [],\n",
        "                \"fold_num\": [],\n",
        "                \"y_true\": [],\n",
        "                \"y_pred\": []\n",
        "            }\n",
        "\n",
        "            # 动态添加概率字段（根据 num_class）\n",
        "            for i in range(1, num_class + 1):\n",
        "                predictions_data[f\"proba_{i}\"] = []\n",
        "            \n",
        "            # 记录实验的开始时间\n",
        "            start_time = time.time()\n",
        "\n",
        "            # 实验循环代码\n",
        "            for batch_num in range(1, 2):\n",
        "                print(f\"开始第 {batch_num} 次模拟实验\")\n",
        "\n",
        "                # 数据路径\n",
        "                sim_path = f\"F:/r-env/data/benchmark-TCGA\"\n",
        "\n",
        "                omics1 = pd.read_csv(f\"{sim_path}/{cancer_type}/exp.csv\", header = 0)\n",
        "                omics2 = pd.read_csv(f\"{sim_path}/{cancer_type}/meth.csv\", header = 0)\n",
        "                omics3 = pd.read_csv(f\"{sim_path}/{cancer_type}/mirna.csv\", header = 0)\n",
        "\n",
        "                omics1 = omics1.iloc[:, 1:].to_numpy() \n",
        "                omics2 = omics2.iloc[:, 1:].to_numpy() \n",
        "                omics3 = omics3.iloc[:, 1:].to_numpy() \n",
        "\n",
        "                df_label = pd.read_csv(f\"{sim_path}/{cancer_type}/label.csv\")\n",
        "                labels = df_label['label'].values \n",
        "                \n",
        "                omics_data = [omics1, omics2, omics3]\n",
        "\n",
        "                for fold_num, (train_idx_raw, test_idx) in enumerate(kfold.split(omics1, labels)):\n",
        "                    print(f\"CV{fold_num + 1}\")\n",
        "\n",
        "                    # 将train_idx_raw按3:1划分为train_idx和val_idx\n",
        "                    train_idx, val_idx = train_test_split(\n",
        "                        train_idx_raw, \n",
        "                        test_size=0.20,  # 25%作为验证集，75%作为训练集 (3:1比例)\n",
        "                        random_state=42,\n",
        "                        stratify=labels[train_idx_raw]  # 保持标签分布\n",
        "                    )\n",
        "\n",
        "                    \n",
        "                    data_tr_list = []\n",
        "                    data_val_list = []\n",
        "                    data_te_list = []\n",
        "                    data_trval_list = []\n",
        "                    data_trte_list = []\n",
        "\n",
        "                    # 通过循环处理每个omics数据集\n",
        "                    for omic in omics_data:\n",
        "                        train_X = omic[train_idx]\n",
        "                        val_X = omic[val_idx]\n",
        "                        test_X = omic[test_idx]\n",
        "                        data_tr_list.append(torch.FloatTensor(train_X).cuda() )\n",
        "                        data_val_list.append(torch.FloatTensor(val_X).cuda() )\n",
        "                        data_te_list.append(torch.FloatTensor(test_X).cuda() )\n",
        "                        data_trval_list.append(torch.FloatTensor(np.concatenate((train_X, val_X), axis=0)).cuda() )\n",
        "                        data_trte_list.append(torch.FloatTensor(np.concatenate((train_X, test_X), axis=0)).cuda() )\n",
        "\n",
        "                    train_y, val_y, test_y = labels[train_idx], labels[val_idx], labels[test_idx]\n",
        "\n",
        "                    num_tr = data_tr_list[0].shape[0]\n",
        "                    num_trval = data_trval_list[0].shape[0]\n",
        "                    num_trte = data_trte_list[0].shape[0]\n",
        "\n",
        "                    labels_trval = np.concatenate((train_y, val_y))\n",
        "                    labels_trte = np.concatenate((train_y, test_y))\n",
        "\n",
        "                    trval_idx = {\n",
        "                        \"tr\": list(range(num_tr)),\n",
        "                        \"te\": list(range(num_tr, num_trval))\n",
        "                    }\n",
        "                    \n",
        "                    trte_idx = {\n",
        "                        \"tr\": list(range(num_tr)),\n",
        "                        \"te\": list(range(num_tr, num_trte))\n",
        "                    }\n",
        "                    \n",
        "                    labels_tr_tensor = torch.LongTensor(labels_trval[trval_idx[\"tr\"]])\n",
        "                    onehot_labels_tr_tensor = one_hot_tensor(labels_tr_tensor, num_class)\n",
        "                    sample_weight_tr = torch.FloatTensor(cal_sample_weight(labels_trval[trval_idx[\"tr\"]], num_class))\n",
        "                    \n",
        "                    if cuda:\n",
        "                        labels_tr_tensor = labels_tr_tensor.cuda()\n",
        "                        onehot_labels_tr_tensor = onehot_labels_tr_tensor.cuda()\n",
        "                        sample_weight_tr = sample_weight_tr.cuda()\n",
        "\n",
        "                    dim_list = [x.shape[1] for x in data_tr_list]\n",
        "\n",
        "                    adj_tr_list, adj_val_list = gen_trte_adj_mat(data_tr_list, data_trval_list, trval_idx, adj_parameter)\n",
        "\n",
        "                    # 初始化模型\n",
        "                    model_dict = init_model_dict(num_view, num_class, dim_list, dim_he_list, dim_hvcdn, dropout_rate)\n",
        "                    optim_dict = init_optim(num_view, model_dict, lr_e_pretrain, lr_c)\n",
        "                    \n",
        "                    for model in model_dict.values():\n",
        "                        if cuda:\n",
        "                            model.cuda()\n",
        "\n",
        "                    \n",
        "                    # 记录单个fold的开始时间\n",
        "                    fold_start_time = time.time()\n",
        "\n",
        "\n",
        "                    # 预训练\n",
        "                    for epoch in range(num_epoch_pretrain):\n",
        "                        train_epoch(data_tr_list, adj_tr_list, labels_tr_tensor, \n",
        "                                    onehot_labels_tr_tensor, sample_weight_tr, model_dict, optim_dict, train_VCDN=True)\n",
        "                    \n",
        "                    # 主训练\n",
        "                    optim_dict = init_optim(num_view, model_dict, lr_e, lr_c)\n",
        "\n",
        "                    # 早停机制\n",
        "                    best_accuracy = 0.0\n",
        "                    patience = 25\n",
        "                    no_improvement_count = 0\n",
        "                    best_model = None\n",
        "\n",
        "\n",
        "                    for epoch in range(num_epoch):\n",
        "                        train_epoch(data_tr_list,adj_tr_list, labels_tr_tensor, \n",
        "                                onehot_labels_tr_tensor, sample_weight_tr, model_dict, optim_dict, train_VCDN=True)\n",
        "                        \n",
        "                        # 每10个epoch验证一次\n",
        "                        if epoch % 2 == 0:\n",
        "                            val_prob = test_epoch(data_trval_list, adj_val_list,trval_idx[\"te\"], model_dict) ####\n",
        "                            predictions = np.argmax(val_prob, axis=1)\n",
        "                            accuracy = accuracy_score(val_y, predictions)\n",
        "                            \n",
        "                            if accuracy > best_accuracy:\n",
        "                                best_accuracy = accuracy\n",
        "                                no_improvement_count = 0\n",
        "                                best_model = copy.deepcopy(model_dict)\n",
        "                            else:\n",
        "                                no_improvement_count += 2\n",
        "                                \n",
        "                            if no_improvement_count >= patience:\n",
        "                                break\n",
        "                            # 记录单个fold的结束时间\n",
        "                    # log_gpu_memory()\n",
        "                    fold_end_time = time.time()\n",
        "                    fold_elapsed_time = fold_end_time - fold_start_time\n",
        "\n",
        "                    memory_allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "\n",
        "                    # 最终测试\n",
        "                    adj_tr_list, adj_te_list = gen_trte_adj_mat(data_tr_list, data_trte_list, trte_idx, adj_parameter)\n",
        "                    predictions = test_epoch(data_trte_list, adj_te_list, trte_idx[\"te\"], best_model)\n",
        "                    y_pred = np.argmax(predictions, axis=1)\n",
        "                    # y_true = labels_trte[trte_idx[\"te\"]]    \n",
        "                    y_true = test_y\n",
        "\n",
        "\n",
        "                    prediction_to_add = {\n",
        "                        \"batch_num\": batch_num,\n",
        "                        \"fold_num\": fold_num + 1,\n",
        "                        \"y_true\": y_true.tolist(),\n",
        "                        \"y_pred\": y_pred.tolist()\n",
        "                    }\n",
        "\n",
        "                    # 动态添加概率预测\n",
        "                    for i in range(num_class):\n",
        "                        prediction_to_add[f\"proba_{i+1}\"] = predictions[:, i].tolist()\n",
        "\n",
        "                    # 动态添加概率预测\n",
        "                    for key, values in prediction_to_add.items():\n",
        "                        if key in [\"batch_num\", \"fold_num\"]:\n",
        "                            predictions_data[key].extend([values] * len(y_true))\n",
        "                        else:\n",
        "                            predictions_data[key].extend(values)  \n",
        "            \n",
        "                    scores = evaluate_model(y_true, y_pred, predictions, num_class)\n",
        "\n",
        "                    for key in metrics:\n",
        "                        if key == \"Batch\":\n",
        "                            metrics[key].append(batch_num)\n",
        "                        elif key == \"CV\":\n",
        "                            metrics[key].append(fold_num + 1)\n",
        "                        elif key == \"Training_time\":\n",
        "                            metrics[key].append(fold_elapsed_time)\n",
        "                        elif key == \"GPU_memory_allocated\":\n",
        "                            metrics[key].append(memory_allocated)\n",
        "                        else:\n",
        "                            metrics[key].append(scores.get(key, None))\n",
        "\n",
        "            end_time = time.time() \n",
        "\n",
        "            trainable_params = count_trainable_parameters(model_dict)\n",
        "            \n",
        "            metrics_df = pd.DataFrame(metrics)\n",
        "            float_cols = metrics_df.select_dtypes(include=['float64']).columns\n",
        "            metrics_df[float_cols] = metrics_df[float_cols].round(6)\n",
        "            result_filepath = os.path.join(result_path, f\"{cancer_type}/metrics-{nn_type}-{cancer_type}.csv\")\n",
        "            metrics_df.to_csv(result_filepath, index=False)\n",
        "\n",
        "            predictions_data = pd.DataFrame(predictions_data)\n",
        "            if not predictions_data.empty:\n",
        "                float_cols = [f'proba_{i+1}' for i in range(num_class)]\n",
        "                predictions_data[float_cols] = predictions_data[float_cols].round(4)\n",
        "            predictions_filepath = os.path.join(prediction_path, f\"{cancer_type}/prediction-{nn_type}-{cancer_type}.csv\")\n",
        "            predictions_data.to_csv(predictions_filepath, index=False)\n",
        "            print(f\"当前癌症类型 {cancer_type} 实验完成，结果已保存到: {result_filepath}\")\n",
        "\n",
        "            elapsed_time = end_time - start_time\n",
        "            run_times.append({\n",
        "                \"cancer_type\": cancer_type,\n",
        "                \"num_class\": num_class,\n",
        "                \"training_num\": batch_num * 4,\n",
        "                \"elapsed_time\": elapsed_time,   \n",
        "                \"trainable_params\": trainable_params\n",
        "            })\n",
        "\n",
        "            # 将所有运行时间保存到 CSV 文件\n",
        "            run_times_df = pd.DataFrame(run_times)\n",
        "            result_filepath = f\"{result_path}/{cancer_type}/{nn_type}_{cancer_type}_实验运行时间.csv\"\n",
        "            run_times_df.to_csv(result_filepath, index=False)\n",
        "\n",
        "            print(f\"癌症类型{cancer_type}实验完成，运行时间已保存到: {result_filepath}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}